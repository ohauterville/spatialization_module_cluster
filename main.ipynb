{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-19 14:20:14.221979\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "print(current_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio Temporal Logistic\n",
    "This notebook is the first test of a spatio temporal logistic to link properly the **built-up surface** of a given region depending on its quality of life (**GDP/cap**) and of its **population density**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Region import Region\n",
    "from Df import Df\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "from colorama import Fore, Style# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 : Theory\n",
    "First we define some classical functions that will be used later for our modelisation.\n",
    "\n",
    "### The logistic function\n",
    "$f(t) = \\frac{S}{1+e^{(-k(t-t_0))}}$\n",
    "\n",
    "with: \n",
    "- *S* : the saturation level\n",
    "- *k* : the slope\n",
    "- $t_0$ : the half height value (left right alignement).\n",
    "\n",
    "The logistic function serves as a predictive equation and as the motor of our dynamical models. Indeed, we observe that at the country scale, as time passes, the GDP/cap increases and some observables as the number of cars or as the number of m2/cap. This means that the quality of life, and thus the stocks consumed by a region improve. But this exponential growth does not increase indefinetely. A saturation is observed even if the GDP/cap still increase. This is explained because people usually do not own 2 or 3 cars even though they possess the money to. An logistic function, also known as the S curve, is a way to model this observation.\n",
    "\n",
    "### The exponential decay function\n",
    "$f(x) = a\\, e^{-b\\,x}+c$\n",
    "\n",
    "with:\n",
    "- *a* : the slope\n",
    "- *b* : the half-life\n",
    "- *c* : the bias.\n",
    "\n",
    "The exponential decay is a decreasing exponential observed in nature (for example for the probability of nuclear decay over time). This is one of our assumption to model the decreasing phenomena observed betwenn the **built-up surface/cap** and the **population density** of a region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x, a, b, c):\n",
    "    return a / (1 + np.exp(-b*(x-c)))\n",
    "\n",
    "def exponential_decay(X, a, b, c):\n",
    "    return a * np.exp(-b * X) + c\n",
    "\n",
    "def exponential_decay2(X, a, b):\n",
    "    return a * np.exp(-b * X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spatio Temporal Logistic function\n",
    "$f(t,x) = \\frac{a\\,e^{-b\\,x}+c}{1 + e^{(-k(t-t_0))}}$\n",
    "\n",
    "The Spatio Temporal Logistic function (**STL**) is the mix between our classical logistic expression and the exponential decay as S (the saturation level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STL(X, a, b, c, d, e):\n",
    "    # STL stands for Spatio Temporal Logistic. It is an refinment of a logistic, dependant on time (here the GDP per capita serves \n",
    "    # as proxy for time), with a saturation level which is a function of space (here the population density of a region)\n",
    "    x1, x2 = X\n",
    "    saturation = exponential_decay(x2, a, b,c)\n",
    "    return saturation / (1 + np.exp(-d*(x1-e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 : Initialisation\n",
    "Initialisation of the analysis parameters.\n",
    "\n",
    "- **region_names** : (*string*) the country to study, named by their ISO3 \n",
    "- **years** : (*string*) years to study\n",
    "- **raster_S** : (*string*) letter used in the **GHSL** dataset (S, S_NRES, POP, ...)\n",
    "- **lvl** : (*int*) the level of our administrative data (GDP and population)\n",
    "- **subregion_borders** : (*string*) the path to administrative border shapefile to cut the subregions \n",
    "- **i dentifier** : (*string*) the column name to match the region names between the administrative data and the GIS data\n",
    "\n",
    "To each region is associated a **DataFrame** (*oecd_DF_merged*) with the subregions matching administrative observables (GDP, population, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl = 1\n",
    "\n",
    "# GHSL type\n",
    "raster_str = \"Built_S\"\n",
    "with_parents_computation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/data/mineralogie/hautervo/data/\"\n",
    "folder_GHSL_S = data_folder + \"Outputs/GHSL/Built_S/TL\" + str(lvl) + \"/\"\n",
    "folder_GHSL_POP = data_folder + \"Outputs/GHSL/POP/TL\" + str(lvl) + \"/\"\n",
    "folder_OSM_building = data_folder + \"Outputs/OSM/building/TL\" + str(lvl) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make the OECD complete DF\n",
    "# oecd_gdp_per_cap_file = data_folder + r\"OECD/GDP per capita/TL\" + str(lvl) + r\"/Gross Domestic Product per capita, in USD.csv\"\n",
    "# oecd_population_file = data_folder + r\"OECD/Population/TL\" + str(lvl) + r\"/Resident population.csv\"\n",
    "\n",
    "# oecd_gdp_per_capita_df = pd.read_csv(oecd_gdp_per_cap_file, skiprows=0, header=1)\n",
    "# oecd_population_df = pd.read_csv(oecd_population_file, skiprows=0, header=1)\n",
    "\n",
    "# df_list = []\n",
    "# df_list.append(oecd_gdp_per_capita_df)\n",
    "# df_list.append(oecd_population_df)\n",
    "\n",
    "# subregion_col = \"tl\"+str(lvl)+\"_id\"\n",
    "# parent_col = \"iso3\"\n",
    "\n",
    "# for df in df_list:\n",
    "#     df[subregion_col] = df[\"Code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "Failed to open dataset (flags=68): /data/mineralogie/hautervo/data/OECD/admin_units/TL1/OECD_TL1_2020_ESRI54009.shp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/ogrext.pyx:130\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:134\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:375\u001b[0m, in \u001b[0;36mfiona._err.StackChecker.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /data/mineralogie/hautervo/data/OECD/admin_units/TL1/OECD_TL1_2020_ESRI54009.shp: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The OECD admin units\u001b[39;00m\n\u001b[1;32m      3\u001b[0m oecd_admin_units \u001b[38;5;241m=\u001b[39m data_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOECD/admin_units/TL\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lvl) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/OECD_TL\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lvl) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_2020_ESRI54009.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m gpd_oecd_admin_units \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43moecd_admin_units\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/geopandas/io/file.py:289\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/geopandas/io/file.py:315\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    316\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/fiona/env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/fiona/__init__.py:342\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, opener, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         path \u001b[38;5;241m=\u001b[39m _parse_path(fp)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 342\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwkt_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwkt_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    357\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[1;32m    358\u001b[0m         path,\n\u001b[1;32m    359\u001b[0m         mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    373\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/fiona/collection.py:226\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:876\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: Failed to open dataset (flags=68): /data/mineralogie/hautervo/data/OECD/admin_units/TL1/OECD_TL1_2020_ESRI54009.shp"
     ]
    }
   ],
   "source": [
    "# The OECD admin units\n",
    "\n",
    "oecd_admin_units = data_folder + \"OECD/admin_units/TL\" + str(lvl) + \"/OECD_TL\" + str(lvl) + \"_2020_ESRI54009.shp\"\n",
    "gpd_oecd_admin_units = gpd.read_file(oecd_admin_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries to ignore from our study (not enough data)\n",
    "country_to_pop = [\"SRB\", \"CRI\", \"ISR\", \"CYP\", \"ISL\", \"ALB\", \"LIE\",\"MNE\",\"MKD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_names = list(oecd_gdp_per_capita_df[\"Country\"].unique())\n",
    "\n",
    "# #exclude some countries if necessary\n",
    "# for c in country_to_pop:\n",
    "#     regions_names.pop(regions_names.index(c))\n",
    "\n",
    "regions_names = [\"FRA\", \"DEU\", \"GBR\", \"BEL\", \"ITA\", \"LUX\", \"ESP\", \"USA\", \"JPN\", \"CAN\", \"AUS\"] # to remove\n",
    "# regions_names = [\"FRA\", \"DEU\", \"GBR\", \"BEL\", \"ITA\", \"LUX\", \"ESP\"] # to remove\n",
    "# regions_names = [\"FRA\"]\n",
    "\n",
    "years = [\"1975\", \"1990\", \"2000\", \"2010\", \"2020\"] \n",
    "# years = [\"2000\", \"2010\", \"2020\"]\n",
    "# years = [\"2020\"]\n",
    "\n",
    "regions = []\n",
    "\n",
    "for name in regions_names:\n",
    "    new_region = Region(name, lvl-1)\n",
    "    new_region.parent_name = name\n",
    "    new_region.subregions.append(new_region)\n",
    "    regions.append(new_region)\n",
    "\n",
    "    for y in years:        \n",
    "        new_region.add_gis(data_folder + \"GHSL/\"+ raster_str + \"/E\" + y + \"_100m_Global/subregions/\" + name + \".tif\", raster_str + \"_\" + y, str(y), lvl-1) \n",
    "        new_region.add_gis(data_folder + \"GHSL/Built_POP/E\" + y + \"_100m_Global/subregions/\" + name + \".tif\", \"POP_\" + y, str(y), lvl-1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 : Computation of the observables\n",
    "Now that we define all the parameters of our study, we will cut the regions into their respective subregions (*make_subregions*). \n",
    "\n",
    "We then compute for each subregions GIS, the geographical observables that we store in its oecd_DF_merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GHSL_values(subregion, folder: str, type: str, overwrite):    \n",
    "    csv_path = os.path.join(folder, subregion.parent_name, subregion.name, '_'.join(years))+\".csv\"\n",
    "    if not os.path.isfile(csv_path) or overwrite:\n",
    "        # S\n",
    "        if type == \"S\":\n",
    "            output_df = pd.DataFrame({\"year\": years, \"Built up surface GHSL\":None, \"Total surface\":None, \"Built up surface fraction\":None})\n",
    "\n",
    "            for y in years:\n",
    "                # first use the Built_S gis\n",
    "                gis = next((gis for gis in subregion.gis_list if gis.name == raster_str + \"_\" + y), None)\n",
    "                if gis != None:\n",
    "                    output_df.loc[output_df[\"year\"]==y, \"Built up surface GHSL\"] = int(gis.get_total_sum_pixel_values())\n",
    "                    output_df.loc[output_df[\"year\"]==y, \"Total surface\"] = int(1e4*gis.get_total_number_pixels())\n",
    "                    output_df.loc[output_df[\"year\"]==y, \"Built up surface fraction\"] = output_df.loc[output_df[\"year\"]==y, \"Built up surface GHSL\"] / output_df.loc[output_df[\"year\"]==y, \"Total surface\"]\n",
    "                else:\n",
    "                    print(Fore.RED, f\"{subregion.name} \", raster_str + \"_\" + y, \" not found.\", Style.RESET_ALL)\n",
    "\n",
    "            # save the new df\n",
    "            os.makedirs(os.path.dirname(csv_path), exist_ok=\"True\")\n",
    "            output_df.to_csv(csv_path, index=False)\n",
    "            subregion.output_df_list.append(Df(output_df, type))\n",
    "\n",
    "        # POP    \n",
    "        elif type == \"POP\":\n",
    "            output_df = pd.DataFrame({\"year\": years, \"Population\":None})\n",
    "\n",
    "            for y in years:\n",
    "                # first use the Built_S gis\n",
    "                gis = next((gis for gis in subregion.gis_list if gis.name == \"POP_\" + y), None)\n",
    "                if gis != None:\n",
    "                    output_df.loc[output_df[\"year\"]==y, \"Population\"] = int(gis.get_total_sum_pixel_values())\n",
    "                else:\n",
    "                    print(Fore.RED, f\"{subregion.name} \", \"POP_\" + y, \" not found.\", Style.RESET_ALL)\n",
    "\n",
    "            # save the new df\n",
    "            os.makedirs(os.path.dirname(csv_path), exist_ok=\"True\")\n",
    "            output_df.to_csv(csv_path, index=False)\n",
    "            subregion.output_df_list.append(Df(output_df, type))\n",
    "\n",
    "        # If we fall here, something wrong happened    \n",
    "        else:\n",
    "            print(f\"Type of GHSL data to compute : {type} not understood.\")\n",
    "    else:\n",
    "        #use a precomputed csv\n",
    "        subregion.output_df_list.append(Df(pd.read_csv(csv_path), type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OSM_areas(subregion, folder: str, overwrite=False): \n",
    "    gis_name = \"OSM_building\"  \n",
    "    csv_path = os.path.join(folder, subregion.parent_name, subregion.name,  subregion.name +\".csv\")\n",
    "\n",
    "    if not os.path.isfile(csv_path) or overwrite:\n",
    "        gis = next((gis for gis in subregion.gis_list if gis.name == gis_name), None)\n",
    "        if gis is not None:\n",
    "            try:\n",
    "                shp = gpd.read_file(gis.file)\n",
    "                print(\"Starting area OSM \", gis.file)\n",
    "                value = (shp[\"geometry\"].area).sum()\n",
    "                output_df = pd.DataFrame({gis_name+\"_area\":value}, index=[0])\n",
    "\n",
    "                # save the new df\n",
    "                os.makedirs(os.path.dirname(csv_path), exist_ok=\"True\")\n",
    "                output_df.to_csv(csv_path, index=False)\n",
    "                subregion.output_df_list.append(Df(output_df, gis_name))\n",
    "                print(\"Ending area OSM \", gis.file)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(f\"OSM shp for {subregion.name} not found.\")\n",
    "    else:\n",
    "        #use a precomputed csv\n",
    "        subregion.output_df_list.append(Df(pd.read_csv(csv_path), gis_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess with the desired computational methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    # Step 1 : make subregions\n",
    "    overwrite = False\n",
    "\n",
    "    # print(Fore.GREEN + \"Starting make_subregions()\" + Style.RESET_ALL)\n",
    "    # with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #     list(executor.map(lambda region: region.make_subregions(gpd_oecd_admin_units, subregion_col, parent_col, overwrite=overwrite), regions))\n",
    "\n",
    "    # Step 2.1 : Computation\n",
    "    overwrite = False\n",
    "\n",
    "    subregions_list_parallel = regions\n",
    "\n",
    "    # for region in regions:\n",
    "    #     for subregion in region.subregions:\n",
    "    #         subregions_list_parallel.append(subregion)\n",
    "    #         try:\n",
    "    #             file = os.path.join(folder_OSM_building, subregion.parent_name, subregion.name, subregion.name +\".shp\")\n",
    "    #             if os.path.isfile(file):\n",
    "    #                 subregion.add_gis(file, \"OSM_building\", \"\", \"\")\n",
    "    #             else:\n",
    "    #                 print(f\"File {file} not found.\")\n",
    "    #         except Exception as e:\n",
    "    #             print(e)\n",
    "\n",
    "    print(Fore.GREEN + \"Starting GHSL_S\" + Style.RESET_ALL)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(get_GHSL_values, subregions_list_parallel, itertools.repeat(folder_GHSL_S), itertools.repeat(\"S\"), itertools.repeat(overwrite))\n",
    "    print(Fore.GREEN + \"Starting GHSL_POP\" + Style.RESET_ALL)   \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(get_GHSL_values, subregions_list_parallel, itertools.repeat(folder_GHSL_POP), itertools.repeat(\"POP\"), itertools.repeat(overwrite))\n",
    "\n",
    "    # print(Fore.GREEN + \"Starting OSM_area_computation()\" + Style.RESET_ALL)\n",
    "    # with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #     executor.map(get_OSM_areas, subregions_list_parallel, itertools.repeat(folder_OSM_building), itertools.repeat(overwrite))\n",
    "\n",
    "    del subregions_list_parallel # not needed anymore\n",
    "\n",
    "    if with_parents_computation:\n",
    "        print(Fore.GREEN + \"Starting computing region DF\" + Style.RESET_ALL)\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            executor.map(lambda region: region.compute_own_df(years, \"GHSL_OECD\"), regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStarting GHSL_S\u001b[0m\n",
      "\u001b[32mStarting GHSL_POP\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "preprocess()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m Ended normally. \u001b[0m\n",
      "2024-11-19 14:59:44.744321\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "print(Fore.GREEN, \"Ended normally.\", Style.RESET_ALL)\n",
    "print(current_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for region in regions:\n",
    "#     for subregion in region.subregions:\n",
    "#         print(len(subregion.output_df_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
